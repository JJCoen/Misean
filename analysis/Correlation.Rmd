---
title: "Modelling with tidymodels"
type: inverse
subtitle: "Linear Regression to model power plant output"
author: "David Svancer (George Mason Uni) adapted by Jim Coen"
date: "`r format(Sys.Date(), '%A, %B %d, %Y') `"
output:
  html_document: 
    toc: yes
    fig.width: 4
    fig_caption: yes
    number_sections: yes
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(echo = TRUE)
options(digits = 3)

# Helper packages
library(readxl)   # excel files
library(readr)    # write csv
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(gridExtra) # multiple plots
library(visdat)   # for additional visualizations
library(data.table) # primary data type for tabular data
library(kableExtra) # kable for printing tabular data
library(skimr)    # Detailed summary of features and values

# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
library(tidyverse)  # data manipulation and visualization
library(tidymodels) # data split
```

# Purpose

We are trying to predict power output given a set of readings from various
sensors in a gas-fired power generation plant. Power generation is a complex
process, and understanding and predicting power output is an important element
in managing a plant and its connection to the power grid.

A combined cycle power plant (CCPP) is composed of gas turbines (GT), steam
turbines (ST) and heat recovery steam generators. In a CCPP, the electricity is
generated by gas and steam turbines, which are combined in one cycle, and is
transferred from one turbine to another. While the Vacuum is colected from and
has effect on the Steam Turbine, he other three of the ambient variables effect
the GT performance.

## Business Understanding

1.  What is the effect of each sensor on power output?
2.  Is there a critical sensor or threshold values for each sensor in terms of
    power output?
3.  Is it possible to predict, with low error, the power output of a CCPP.

# Data Inspection

## Data Set Information:

The data-set contains 9568 data points collected from a Combined Cycle Power
Plant over 6 years (2006-2011), when the power plant was set to work with full
load. Features consist of hourly average ambient variables Temperature (T),
Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict
the net hourly electrical energy output (EP) of the plant.

### Data Dictionary

\- Temperature (T) in the range 1.81°C and 37.11°C,\
- Ambient Pressure (AP) in the range 992.89-1033.30 milibar,\
- Relative Humidity (RH) in the range 25.56% to 100.16%\
- Exhaust Vacuum (V) in the range 25.36-81.56 cm Hg\
- Net hourly electrical energy output (EP) 420.26-495.76 MW\
The averages are taken from various sensors located around the plant that record
the ambient variables every second. The variables are given without
normalization.

## Load

Source: [UCI Machine Learning Repository Combined Cycle Power Plant Data
Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)

```{r}
if (!file.exists("./data/power_df.csv")) {
  power_df <- read_excel("./data/Power-Plant/Folds5x2_pp.xlsx")
  write_csv(power_df, file = "./data/power_df.csv")
  setDT(power_df)
} else {
  power_df <- fread("./data/power_df.csv")
}
str(power_df)
```

### Randomly sample training and test sets

1.  Create an `rsample` object, `power_split`, that contains the instructions
    for randomly splitting the `home_sales` data into a training and test
    data-set.
2.  Allocate 70% of the data into training and stratify the results by `PE.`

```{r}
# Create data split object
power_split <- initial_split(power_df, prop = 0.7,
                     strata = PE)

# Create the training data
power_training <- power_split %>% 
  training()

# Create the test data
power_test <- power_split %>% 
  testing()

# Check the number of rows
nrow(power_training)
nrow(power_test)
```

## Target Variable

### **Distribution of outcome variable values**

Stratifying by the outcome variable when generating training and test data-sets
ensures that the outcome variable values have a similar range in both data-sets.

1.  Calculate the minimum, maximum, mean, and standard deviation of the `PE`
    variable

```{r}
# Distribution of PE in training data
power_training %>% 
  summarize(min_power_out = min(PE),
            max_power_out = max(PE),
            mean_power_out = mean(PE),
            sd_power_out = sd(PE)) 
```

```{r}
ggplot(data=power_training, aes(x = PE )) +
  geom_histogram(color="black", fill="white", bins=50) +
  geom_vline(aes(xintercept=mean(PE)), color="blue", linetype="dashed", size=1)
  

```

$\rightarrow$ Target variable is bi-modal.

## Features

### Distribution

Reshape `power_df` to long format with key value pairs

```{r, include=FALSE}
power_training[, -("PE")] %>% gather() 
```

Create histograms

```{r}

ggplot(gather(power_training[, -("PE")]), aes(value)) + 
    geom_histogram(color="black", fill="white", bins=50) +
    facet_wrap(~key, scales = 'free_x')
```

$\rightarrow$ right skew in RH can be reduced with Box-Cox transformation.

$\rightarrow$ AP has a normal distribution while AT is approximately normal.

## Relationship between features and target variable

### Check for outliers

```{r, message=FALSE, include=FALSE}
makeScatterplots <- function(dataframe,x.variable, y.variable){
  print(ggplot(dataframe, aes_string(x=x.variable,y= y.variable)) + 
          geom_point(shape=18, color="blue")) +
  geom_smooth(method = "lm", se=FALSE, color="black") 
}


g1 <- makeScatterplots(power_training,"AT","PE")
g2 <- makeScatterplots(power_training,"AP","PE")
g3 <- makeScatterplots(power_training,"RH","PE")
g4 <- makeScatterplots(power_training,"V","PE")
```

```{r, message=FALSE}
grid.arrange(g1, g2, g3, g4, ncol = 2, nrow =2)
```

$\rightarrow$ despite certain points on the edges of the main clusters, there
are no distinct outliers.

### Co-linearity among features

```{r}
# install.packages("corrr")
library(corrr)
power_training %>%
       correlate() %>%    # Create correlation data frame (cor_df)
       # focus(-PE, mirror = TRUE) %>%  # Focus on cor_df without 'PE'
       rearrange() %>%  # rearrange by correlations
       shave() # Shave off the upper triangle for a clean result
```

$\rightarrow$ high negative correlation (-0.95) between atmospheric temp. and
power outpupt.

$\rightarrow$ high correlation (0.84) between atmospheric temp. and exhaust
vacuum.

# Fit a Linear Regression Model

```{r}
# Specify a logistic regression model
linear_model <- linear_reg() %>% 
  # Set the engine
  set_engine('lm') %>% 
  # Set the mode
  set_mode('regression')

# Fit to training data
lm_fit <- linear_model %>% 
  fit(PE ~ AT + V + AP + RH,
      data = power_training)

# Print model fit object
lm_fit
```

## Predicting power output

After fitting a model using the training data, the next step is to use it to
make predictions on the test dataset. The test dataset acts as a new source of
data for the model and will allow you to evaluate how well it performs.

Before you can evaluate model performance, you must add your predictions to the
test data-set.

-   Create a tibble with the `PE` and all predictor columns from the test
    data-set and the predicted home selling prices.

```{r}
# Predict power output
power_predictions <- predict(lm_fit,
                            new_data = power_test)

# View predicted power output
power_predictions %>% head()

# Combine test data with predictions
power_test_results <- power_test %>% 
  bind_cols(power_predictions)

# View results
power_test_results %>% head()
```

# Evaluating Model Performance

## **Root mean squared error (RMSE)**

A common performance metric for regression models is the root mean squared
error, or RMSE. The RMSE estimates the average prediction error of a model and
is calculated with the rmse() function. To calculate the RMSE on our mpg model,
we pass `power_test_results` to the `rmse`() function and specify hwy as the
truth and `.pred` as the estimate. We see that the average prediction error of
our model is about 4.57 MW for the estimated power output values.

## **R squared metric**

Another important regression metric is R squared, also known as the coefficient
of determination.

-   R squared measures the squared correlation between actual and predicted
    values and
-   ranges from 0 to 1, where 1 indicates that all predictions equal the true
    outcome values.
-   R squared is calculated with the `rsq`() function and takes the same
    arguments as the `rmse`() function.

```{r}
# Print home_test_results
power_test_results %>% head()

# Calculate the RMSE metric
power_test_results %>% 
  rmse(truth = PE, estimate = .pred)

# Calculate the R squared metric
power_test_results %>% 
  rsq(truth = PE, estimate = .pred)
```

## **R squared plots**

R squared plots are a way to visualize R squared and consist of a scatter plot
with model predictions on the y-axis and true outcome values on the x-axis.

-   The line y = x is also plotted and represents the case where all predictions
    and outcome values are equal, giving an R squared value of 1.

-   R squared plots are helpful for identifying problems with model performance,
    such as non-linear relationships between the outcome variable and predictors
    or regions where the model may be systematically under or over-predicting
    the outcome value.

-   `coord_obs_pred()` standardises the range on each axis.

```{r}
# Create an R squared plot of model performance
ggplot(power_test_results, aes(x = PE, y = .pred)) +
  geom_point(alpha = 0.5) + 
  geom_abline(color = 'blue', linetype = 2) +
  coord_obs_pred() +
  labs(x = 'Actual Power Output', y = 'Predicted Power Output')
```

$\rightarrow$ There are a few outliers where the model over-predicts power
output.

## **Streamlining model fitting**

The `last_fit`() function is used to streamline the model fitting and evaluation
process in tidymodels.

-   It takes a parnsip model object,
-   model formula, and
-   rsample data split object

and performs the following steps.

1.  It creates training and test datasets,
2.  fits the model to the training data,
3.  calculates metrics and predictions on the test data, and
4.  returns an object with all the results.

To fit our linear regression model on the mpg data, we pass the `lm_model`
parsnip object to `last_fit`(), specify our model formula, and provide the
mpg_split data split object.

```{r}
# Define a linear regression model
linear_model <- linear_reg() %>% 
  set_engine('lm') %>% 
  set_mode('regression')

# Train linear_model with last_fit()
linear_fit <- linear_model %>% 
  last_fit(PE ~ ., split = power_split)

# Collect predictions and view results
predictions_df <- linear_fit %>% collect_predictions()
predictions_df %>% head()

# Make an R squared plot using predictions_df
ggplot(predictions_df, aes(x = PE, y = .pred)) + 
  geom_point(alpha = 0.5) + 
  geom_abline(color = 'blue', linetype = 2) +
  coord_obs_pred() +
  labs(x = 'Actual Power Output', y = 'Predicted Power Output')
```

## **Collecting metrics**

Once the model is trained with the last_fit() function, we pass the lm_last_fit
object to the collect_metrics() function to get a tibble with calculated metrics
on the test data. The default metrics for regression models are RMSE and R
squared and are always stored in the column named dot-estimate. We get the same
performance metrics on the mpg_test data as before, just with a lot less work!

```{r}
linear_fit %>% collect_metrics()
```
